name: Chaos Engineering Tests

on:
  schedule:
    # Run chaos tests daily at 2 AM UTC (during low traffic)
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      intensity:
        description: 'Chaos test intensity'
        required: true
        default: 'low'
        type: choice
        options:
          - low
          - medium
          - high
      categories:
        description: 'Test categories (comma-separated)'
        required: false
        default: 'network,resource,service'
      duration:
        description: 'Test duration in minutes'
        required: false
        default: '15'

jobs:
  chaos-tests:
    runs-on: ubuntu-latest
    
    permissions:
      contents: read
      issues: write
      pull-requests: write
    
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/pip
          ~/.local
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}-chaos
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libldap2-dev \
          libsasl2-dev \
          iperf3 \
          tc \
          stress-ng \
          iotop \
          sysstat
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-asyncio pytest-timeout pytest-xdist
    
    - name: Start test infrastructure
      run: |
        # Start any required services
        docker-compose -f tests/docker-compose.test.yml up -d
        
        # Wait for services to be ready
        sleep 10
    
    - name: Run pre-flight checks
      run: |
        # Verify system is in good state before chaos
        python -m pytest tests/chaos/test_preflight.py -v || true
    
    - name: Execute chaos experiments
      env:
        CHAOS_INTENSITY: ${{ github.event.inputs.intensity || 'low' }}
        CHAOS_CATEGORIES: ${{ github.event.inputs.categories || 'network,resource,service' }}
        CHAOS_DURATION: ${{ github.event.inputs.duration || '15' }}
      run: |
        # Run chaos test runner
        python tests/chaos/chaos_runner.py \
          --intensity $CHAOS_INTENSITY \
          --categories $(echo $CHAOS_CATEGORIES | tr ',' ' ') \
          --duration $CHAOS_DURATION \
          --config tests/chaos/ci_config.yaml
    
    - name: Run chaos test suite
      timeout-minutes: 60
      run: |
        # Run pytest chaos tests with appropriate markers
        python -m pytest tests/chaos/ \
          -v \
          -m "chaos_${{ github.event.inputs.intensity || 'low' }}" \
          --tb=short \
          --timeout=300 \
          --junit-xml=chaos-test-results.xml \
          || true
    
    - name: Collect system metrics
      if: always()
      run: |
        # Collect system performance data
        echo "=== System Metrics ==="
        echo "CPU Usage:"
        mpstat -P ALL 1 5
        
        echo -e "\nMemory Usage:"
        free -h
        
        echo -e "\nDisk I/O:"
        iostat -x 1 5
        
        echo -e "\nNetwork Statistics:"
        ss -s
        
        # Save to file for artifact
        {
          echo "Timestamp: $(date)"
          echo "CPU Info:"
          lscpu
          echo -e "\nMemory Info:"
          cat /proc/meminfo
          echo -e "\nDisk Info:"
          df -h
        } > system-metrics.txt
    
    - name: Analyze chaos results
      if: always()
      run: |
        # Parse and analyze chaos test results
        python -c "
import json
import xml.etree.ElementTree as ET
import sys

# Parse JUnit XML results
try:
    tree = ET.parse('chaos-test-results.xml')
    root = tree.getroot()
    
    total_tests = int(root.attrib.get('tests', 0))
    failures = int(root.attrib.get('failures', 0))
    errors = int(root.attrib.get('errors', 0))
    skipped = int(root.attrib.get('skipped', 0))
    
    success_rate = ((total_tests - failures - errors) / total_tests * 100) if total_tests > 0 else 0
    
    print(f'Chaos Test Summary:')
    print(f'  Total Tests: {total_tests}')
    print(f'  Passed: {total_tests - failures - errors - skipped}')
    print(f'  Failed: {failures}')
    print(f'  Errors: {errors}')
    print(f'  Skipped: {skipped}')
    print(f'  Success Rate: {success_rate:.1f}%')
    
    # Set exit code based on results
    if success_rate < 80:
        print('\nChaos tests show system resilience below threshold!')
        sys.exit(1)
    else:
        print('\nSystem demonstrated acceptable resilience to chaos.')
        
except Exception as e:
    print(f'Error analyzing results: {e}')
        "
    
    - name: Upload chaos test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: chaos-test-results-${{ matrix.python-version }}
        path: |
          chaos-test-results.xml
          chaos_reports/
          chaos_runner.log
          system-metrics.txt
    
    - name: Upload test report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: chaos-engineering-report
        path: chaos_reports/
    
    - name: Create issue for failures
      if: failure() && github.event_name == 'schedule'
      uses: actions/github-script@v6
      with:
        script: |
          const date = new Date().toISOString().split('T')[0];
          const title = `Chaos Engineering Test Failure - ${date}`;
          
          const body = `
          ## Chaos Engineering Test Failure
          
          The scheduled chaos engineering tests failed on ${date}.
          
          **Test Configuration:**
          - Intensity: ${{ github.event.inputs.intensity || 'low' }}
          - Categories: ${{ github.event.inputs.categories || 'network,resource,service' }}
          - Duration: ${{ github.event.inputs.duration || '15' }} minutes
          
          **Action Required:**
          1. Review the test artifacts in the workflow run
          2. Analyze system resilience gaps
          3. Implement necessary hardening measures
          
          [View Workflow Run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          `;
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: title,
            body: body,
            labels: ['chaos-engineering', 'test-failure', 'reliability']
          });
    
    - name: Cleanup
      if: always()
      run: |
        # Stop test infrastructure
        docker-compose -f tests/docker-compose.test.yml down || true
        
        # Clean up any remaining chaos artifacts
        sudo pkill -f stress-ng || true
        sudo tc qdisc del dev lo root || true

  analyze-trends:
    needs: chaos-tests
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v4
    
    - name: Analyze chaos test trends
      run: |
        # Analyze trends across Python versions and generate summary
        python -c "
import os
import json
import xml.etree.ElementTree as ET
from pathlib import Path

results = []

# Parse all test results
for artifact_dir in Path('.').glob('chaos-test-results-*'):
    xml_file = artifact_dir / 'chaos-test-results.xml'
    if xml_file.exists():
        tree = ET.parse(xml_file)
        root = tree.getroot()
        
        version = artifact_dir.name.split('-')[-1]
        results.append({
            'python_version': version,
            'total': int(root.attrib.get('tests', 0)),
            'failures': int(root.attrib.get('failures', 0)),
            'errors': int(root.attrib.get('errors', 0))
        })

# Generate summary
if results:
    print('## Chaos Engineering Test Summary Across Python Versions\n')
    print('| Python | Total | Passed | Failed | Success Rate |')
    print('|--------|-------|--------|--------|--------------|')
    
    for r in sorted(results, key=lambda x: x['python_version']):
        total = r['total']
        failed = r['failures'] + r['errors']
        passed = total - failed
        rate = (passed / total * 100) if total > 0 else 0
        print(f\"| {r['python_version']} | {total} | {passed} | {failed} | {rate:.1f}% |\")
        "
    
    - name: Post summary to PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          // Read summary if generated
          let summary = 'Chaos engineering tests completed. Check artifacts for detailed results.';
          
          github.rest.issues.createComment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
            body: summary
          });